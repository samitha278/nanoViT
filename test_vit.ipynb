{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8keVon7tArYxrcaAevEn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/nanoViT/blob/main/test_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zfoCb8ajyB82"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT Model"
      ],
      "metadata": {
        "id": "-WT1HUrWyWhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ViTConfig:\n",
        "    batch_size: int = 32\n",
        "    num_classes: int = 10\n",
        "    img_size: int = 224\n",
        "    im_channels: int = 3\n",
        "    patch_size: int = 16\n",
        "\n",
        "    n_head: int = 4\n",
        "    n_layer: int = 4\n",
        "    n_embd: int = 1024\n",
        "\n",
        "    @property\n",
        "    def n_patch(self):\n",
        "        return (self.img_size//self.patch_size)**2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "\n",
        "    def __init__(self,config):\n",
        "      super().__init__()\n",
        "\n",
        "\n",
        "      self.config = config\n",
        "\n",
        "      self.embd = PatchEmbedding(config)\n",
        "\n",
        "      self.block = nn.ModuleList([Block(config)  for i in range(config.n_layer)])\n",
        "\n",
        "\n",
        "      self.ln = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "      self.layer = nn.Linear(config.n_embd,config.num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x,targets = None):\n",
        "\n",
        "      B,C,H,W = x.shape\n",
        "\n",
        "\n",
        "      #embedding\n",
        "      out =  self.embd(x)\n",
        "\n",
        "      #blocks\n",
        "      for block in self.block:\n",
        "        out = block(out)\n",
        "\n",
        "      #layer norm\n",
        "      out = self.ln(out)\n",
        "\n",
        "      #linear layer\n",
        "      out = self.layer(out[:,0])\n",
        "\n",
        "\n",
        "      if targets is None:\n",
        "        return out\n",
        "      else:\n",
        "        return F.cross_entropy(out,targets.view(-1))\n",
        "\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.config = config\n",
        "\n",
        "    self.n_patches = config.n_patch\n",
        "    self.patch_dim = config.im_channels* config.patch_size ** 2\n",
        "\n",
        "\n",
        "    #patch embedding\n",
        "    self.patch_embd = nn.Sequential(\n",
        "        nn.LayerNorm(self.patch_dim),\n",
        "        nn.Linear(self.patch_dim,config.n_embd),\n",
        "        nn.LayerNorm(config.n_embd)\n",
        "    )\n",
        "\n",
        "    #cls tokens\n",
        "    self.cls_token = nn.Parameter(torch.randn((config.n_embd,)))\n",
        "\n",
        "    #possitional embedding\n",
        "    self.pos_embd = nn.Embedding(self.n_patches+1,config.n_embd)    # +1 for cls token\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    B,C,H,W = x.shape\n",
        "\n",
        "    # B,C,H,W -> B, n_patches , patch_dim    # patch_dim = C* patch_size*patch_size\n",
        "\n",
        "    patch_size = self.config.patch_size\n",
        "\n",
        "    patches = F.unfold(x, patch_size, stride = patch_size).transpose(-1,-2)\n",
        "\n",
        "    #patch embedding\n",
        "    patch_embd = self.patch_embd(patches)        # B, n_patches , n_embd\n",
        "\n",
        "    #class token\n",
        "    class_tok = self.cls_token.expand(B,1,-1)     # B , 1 , n_embd\n",
        "\n",
        "\n",
        "    patch_embd = torch.cat((class_tok,patch_embd),dim =1 )    # B, n_patches +1  , n_embd\n",
        "\n",
        "    #positional embedding\n",
        "    pos_embd = self.pos_embd(torch.arange(0,self.n_patches+1))     # B, n_patches +1  , n_embd\n",
        "\n",
        "    out = patch_embd + pos_embd\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = Attention(config.n_embd,config.n_head)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config.n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,n_embd):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.layer = nn.Linear(n_embd,4*n_embd)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.proj = nn.Linear(4*n_embd,n_embd)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "    x = self.gelu(self.layer(x))\n",
        "    x = self.proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "#_______________________________________________________________________________\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,n_embd,n_head) :\n",
        "    super().__init__()\n",
        "\n",
        "    self.nh = n_head\n",
        "\n",
        "    self.w = nn.Linear(n_embd,3*n_embd)    # 3 * n_head * head_size\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    wei = self.w(x)        # B,T, 3* C\n",
        "\n",
        "    k,q,v = torch.chunk(wei,3, dim = -1)      # each B,T,C\n",
        "\n",
        "    head_size = C//self.nh\n",
        "\n",
        "    key   = k.view(B, T, self.nh, head_size).transpose(1, 2)    # B, n_head, T, head_size\n",
        "    query = q.view(B, T, self.nh, head_size).transpose(1, 2)    # \"\"\n",
        "    value = v.view(B, T, self.nh, head_size).transpose(1, 2)\n",
        "\n",
        "\n",
        "    weight = ( query @ key.transpose(-1,-2) )  * (head_size ** -0.5)    #B,nh,T,T\n",
        "    weight = F.softmax(weight,dim = -1)\n",
        "\n",
        "    out = weight @ value      #B,nh,T,n_head\n",
        "\n",
        "\n",
        "\n",
        "    out = out.transpose(1,2).view(B,T,C)    #B,T,nh,n_head\n",
        "\n",
        "    out = self.proj(out)\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "xgfVJjrVyWBw"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}